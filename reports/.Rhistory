url = "http://www.mas.ncl.ac.uk/~nseg4/teaching/MAS8380/practical2.dat"
arr.times = scan(url)
arr.times
# Transferring The Data Into Vector Time And Plotting Its Graph
Time = c(1:189)
plot(Time,arr.times,xlab="Patient",ylab="Time")
# Importing Data Of The Patients Arriving In Hours After 9AM Recorded Over A 12 Hour Period.
url = "http://www.mas.ncl.ac.uk/~nseg4/teaching/MAS8380/practical2.dat"
arr.times = scan(url)
# Transferring The Data Into Vector Time And Plotting Its Graph
Time = c(1:189)
plot(Time,arr.times,xlab="Patient",ylab="Time")
# Counting The Number Of Arrivals According To The 10 Equal Length Of Intervals
x = table(cut(arr.times, breaks=10))
# Calculating And Printing The Mean
Mean = mean(x)
print(Mean) #18.9
# Calculating And Printing The Variance
Variance = var(x)
print(Variance) #16.98889
# Calculating The Value Of Lemda IN Correspond To Mean & Variance
Lemda = (Mean*10)/12
print(Lemda) #15.75
Varlemda = (Variance*10)/12
print(Varlemda) #14.15741
# Creating A Vector Of K Equal Length Of Intervals With The Name N
N = c(25,50,75,100,150)
# Creating Another Vector To Store The Value Of Mean & Variance
Mean_Of_i = vector()
Variance_Of_i = vector()
# Staring A FOR Loop To Calculate The Mean & Variance Using The Values Of N As K
for(i in seq_along(N))
{
Table = table(cut(arr.times, breaks = N[i]))
Mean_Of_i[i] = mean(Table)
Variance_Of_i[i] = var(Table)
# Printing The Value Of Mean & Variance
print(Mean_Of_i)
print(Variance_Of_i)
}
# Graph Plot Of Mean vs Variance To Support The Idea Of Arrivals Following The Poisson Process
plot(N, Mean_Of_i, xlab = "K Equal Length Of Interval", ylab = "Mean & Variance",
main = "PLotting K Vs Mean & Variance", type = "o", col = "Orange")
points(N, Variance_Of_i, col = "Blue", type = "o", pch = "Variance")
legend(x="topright", legend = c("MEAN", "VARIANCE"), col = c("Orange", "Blue"),
pch = c("o", "v"), lty = c(1,1), ncol = 1)
# Creating A Simulation For Collecting Points From A Poisson Process
Plot_Function = function(mode){
if(mode=="e"){
Distance = rexp(1000, 10)
}
if(mode=="u"){
Distance = runif(1000, 0, 0.1)
}
cumDistance = cumsum(Distance)
Length_12 = c()
for (j in cumDistance) {
Length_12 = c(Length_12, j)
if(j>=12){
break
}
}
return(Length_12)
}
# Graph Plot For Exponential Growth
Length_12 = Plot_Function("e")
plot(x = seq(length(Length_12)), y = Length_12, xlab = "Patients", ylab = "Time")
Exponential_Plot = (length(Length_12)-1)/(Length_12[length(Length_12)]-Length_12[1])
print(Exponential_Plot)
# Graph Plot For Uniform Growth
U_Length_12 = Plot_Function("u")
plot(x = seq(length(U_Length_12)), y = U_Length_12, xlab = "Patients", ylab = "Time")
Uniform_Plot = (length(U_Length_12)-1)/(U_Length_12[length(U_Length_12)]-U_Length_12[1])
print(Uniform_Plot)
install.packages("mlbench")
library(mlbench)
#loading the data
data("BreastCancer")
#checking the size
dim(BreastCancer)
head(BreastCancer)
na.omit(BreastCancer)
table(BreastCancer$Class)
pairs(BreastCancer[,1:9],col=BreastCancer[,10]+1)
pairs(BreastCancer[,1:9],col=BreastCancer[,10])
pairs(BreastCancer[,1:9], col=BreastCancer[,10])
pairs(BreastCancer[,1:9])
pairs(BreastCancer[,1:8])
pairs(BreastCancer[,2:8])
pairs(BreastCancer[,2:9])
#graphical plot of exploratory dta analysis
pairs(BreastCancer[,2:10])
#picking predictor variables
X1=BreastCancer[,2:10]
#scaling the variable
X1= scale(X1)
#Pick out response variable
y=BreastCancer[,11]
#combining to make a new dtaframe
breast_cancer=data.frame(X1,y)
#storing the value of n and p
n=nrow(breast_cancer)
p=ncol(breast_cancer)-1
logreg_fit=glm(y~., data = breast_cancer, family = "binomial")
#picking predictor variables
X1=BreastCancer[,2:10]
#scaling the variable
X1= scale(X1)
#Pick out response variable
y=BreastCancer[,11]
#combining to make a new dtaframe
breast_cancer=data.frame(X1,y)
#storing the value of n and p
n=nrow(breast_cancer)
p=ncol(breast_cancer)-1
logreg_fit=glm(y~., data = breast_cancer, family = "binomial")
#picking predictor variables
X1=BreastCancer[,2:9]
#scaling the variable
X1= scale(X1)
#Pick out response variable
y=BreastCancer[,11]
#combining to make a new dtaframe
breast_cancer=data.frame(X1,y)
#storing the value of n and p
n=nrow(breast_cancer)
p=ncol(breast_cancer)-1
logreg_fit=glm(y~., data = breast_cancer, family = "binomial")
#picking predictor variables
X1=BreastCancer[,2:10]
#scaling the variable
X1= scale(X1)
#Pick out response variable
y=BreastCancer[,11]
#combining to make a new dtaframe
breast_cancer=data.frame(X1,y)
#storing the value of n and p
n=nrow(breast_cancer)
p=ncol(breast_cancer)-1
logreg_fit=glm(y~., data = breast_cancer, family = "binomial")
summary(logreg_fit)
#numerical exploratory data analysis
summary(BreastCancer)
library(bestglm)
install.packages("Leaps")
yes
install.packages("leaps")
library(bestglm)
bss_fit_AIC=bestglm(breast_cancer, family=binomial,IC="AIC")
library(bestglm)
bss_fit_AIC=bestglm(breast_cancer, family=binomial,IC="AIC")
bss_fit_BIC=bestglm(breast_cancer, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
#examine the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
#identify best-fitting models
(best_AIC=bss_fit_AIC$ModelReport$Bestk)
(best_AIC=bss_fit_BIC$ModelReport$Bestk)
#create multi panel plotting device
par(mfrow=c(1,2))
#Produce plots,highlighting optimal value of K
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab = "Number of predictors", ylab = "AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#identify best-fitting models
(best_AIC=bss_fit_AIC$ModelReport$Bestk)
(best_BIC=bss_fit_BIC$ModelReport$Bestk)
#create multi panel plotting device
par(mfrow=c(1,2))
#Produce plots,highlighting optimal value of K
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab = "Number of predictors", ylab = "AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#identify best-fitting models
(best_AIC=bss_fit_AIC$ModelReport$Bestk)
(best_BIC=bss_fit_BIC$ModelReport$Bestk)
#create multi panel plotting device
par(mfrow=c(1,2))
#Produce plots,highlighting optimal value of K
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab = "Number of predictors", ylab = "AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#create multi panel plotting device
par(mfrow=c(1,2))
#Produce plots,highlighting optimal value of K
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab = "Number of predictors", ylab = "AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#load the glmnet package
library(glmnet)
#choose grid of values for the tuning parameter
grid=10^seq(-4,-1, length.out=100)
#fit a model with lasso penalty for each value of the tuning parameter
lasso_fit = glmnet(X1, y, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
#Examine the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar="lambda",col=rainbow(p), label=TRUE)
install.packages("ProjectTemplate")
#install.packages("ProjectTemplate")
library(ProjectTemplate)
library(ProjectTemplate)
setwd("C:\Users\sagar\OneDrive\Desktop")
create.project("FL")
setwd("C:/Users/sagar/OneDrive/Desktop")
create.project("FL")
setwd("FL/reports")
install.packages("tinytex")
install_tinytex()
